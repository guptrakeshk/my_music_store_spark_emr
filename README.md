## Data Engineering Project Using Spark on AWS EMR 


### A hands-on, Project-based instruction for data engineering ETL using Saprk and AWS EMR

This is a collection of resources for data engineering ETL for a fictious a music company.

Email: gupt.rakeshk@gmail.com

This project walks through end-to-end data engineering steps that are needed in a typical project.
Access event log data from AWS S3 to dynamically read data in data frame, create a temp database and ETL pipeline using Spark for a music streaming app. 
Spark application reads songs and events log data from AWS S3 and load in memory for faster processing. For simpler declarative SQL style queries Spark allows to create a temp table from data frames using Python SQL APIs. After processing and transforming songs and events log data application writes processed data into Apache Parquet file format and store into AWS S3 desired location.

#### Pre-Requisites
- Ensure AWS IAM user is created with read access to S3 as well as necessary access for AWS EMR.
- AWS EMR cluster is up, configured and running 
- AWS IAM user has right security policy set-up to access and perform ETL on AWS EMR.
- Ensure IAM role ARN is tested for right permission to access S3 for reading data and subsequently writing processed data in AWS S3 data lake.
- For security of confidential information, credentials details must be stored in safe configuration file.
- For better network efficiency, ensure EMR cluster is created in the same region where S3 bucket holding events log and songs meta data is located .
- Also ensure the desired AWS S3 location of the data lake is in the same region where EMR cluster is running.

### Steps involved are :
- Copy data that are stored in AWS S3. 
- Apply processing, transformation and subsequent writing to Apache file format using Spark followed with loading processed data into desired AWS S3 data lake.
- Once data is cleansed, transformed and loaded into AWS s3 data lake, it is ready to asnwer queries for analytics.


#### Building ETL Pipeline steps:

- Saprk application accesses S3 bucket to read events log and song data from the EMR cluster.
- Iterate through each event file in event_log data to extract events data
- Extract users, time and songsplay activity data from events after processing
- Create users, time, songsplay data files in Apache parquet format and write back in AWS S3 desired location
- Generate Apache Cassandra CREATE and INSERT statements to load processed records into relevant tables in your data model
- Once data is cleansed, transformed and loaded in AWS S3 data lake, it is ready to asnwer specific queries for analytics.


#### How to execute programs :

Here are helpful steps in executing Spark program.

1. From project workspace CLI/terminal.
- execute `python etl.py` 
2. From EMR cluster
- execute `/usr/bin/spark-submit --master=yarn etl.py`

Note: Make sure spark prgram etl.py is present on the EMR master cluster.




### Project Dataset 

#### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

Song data location : s3://udacity-dend/song_data

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"artist_id":"ARJNIUY12298900C91","artist_latitude":null,"artist_location":"","artist_longitude":null,"artist_name":"Adelitas Way","duration":213.9424,"num_songs":1,"song_id":"SOBLFFE12AF72AA5BA","title":"Scream","year":2009}
```

#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

Log data location : s3://udacity-dend/log_data
Log data json path location: s3://udacity-dend/log_json_path.json

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is an example of what a event log looks like.
```
{"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}
```

Song data schema that is used to enforce schema while reading data from songs data log

```
song_data_schema = StructType([
                StructField("song_id", StringType()),
                StructField("title", StringType(), True),
                StructField("artist_id", StringType(), True),
                StructField("year", IntegerType(), True),
                StructField("duration", DoubleType(), True),
                StructField("artist_name", StringType(), True),
                StructField("artist_location", StringType(), True),
                StructField("artist_latitude", DoubleType(), True),
                StructField("artist_longitude", DoubleType(), True),
                StructField("num_songs", IntegerType(), True)
])
```

### Project artifacts and program

- `etl.py` : A python script to build an ETL pipeline that involves extracting source data from S3, process it using Spark, writing into Apache Parquet file format back in AWS S3 desired location after performing transformation. 



